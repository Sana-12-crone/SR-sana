{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "calling gemini model into colab"
      ],
      "metadata": {
        "id": "QVnQMaLFvtWS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjvOix_CiDux",
        "outputId": "e9e3ed4f-085e-472c-8141-b8f140b7847f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:502: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let's break down how AI works. It's a vast field, so I'll focus on the most common and impactful approaches, emphasizing the core concepts.\n",
            "\n",
            "**What is AI, fundamentally?**\n",
            "\n",
            "At its core, Artificial Intelligence (AI) is about creating computer systems that can perform tasks that typically require human intelligence. These tasks include:\n",
            "\n",
            "*   **Learning:** Acquiring information and rules for use\n",
            "*   **Reasoning:** Using rules to reach conclusions.\n",
            "*   **Problem-solving:** Finding solutions to complex issues.\n",
            "*   **Perception:** Interpreting sensory input (like images, sound, text)\n",
            "*   **Natural Language Processing:** Understanding and generating human language.\n",
            "\n",
            "**The Two Main Approaches to AI:**\n",
            "\n",
            "1.  **Rule-Based AI (Symbolic AI):**\n",
            "\n",
            "    *   **How it Works:**  This approach involves explicitly programming rules and knowledge into the system.  Think of it like a flow chart with lots of \"if-then\" statements.\n",
            "    *   **Example:** A medical diagnosis system where you input symptoms, and the system uses a set of pre-defined rules to determine possible illnesses.\n",
            "    *   **Pros:**\n",
            "        *   Easy to understand and debug.\n",
            "        *   Predictable behavior.\n",
            "        *   Works well in well-defined domains with clear rules.\n",
            "    *   **Cons:**\n",
            "        *   Difficult to scale to complex, real-world problems.\n",
            "        *   Requires significant human effort to create and maintain the rules.\n",
            "        *   Inflexible - can't easily adapt to new information or situations not explicitly covered by the rules.\n",
            "2.  **Machine Learning (ML):**\n",
            "\n",
            "    *   **How it Works:** Instead of programming explicit rules, you provide the system with data, and it *learns* patterns and relationships from that data. The ML model builds up a \"world view\" from the data.\n",
            "    *   **Key Idea:**  The algorithm adjusts its internal parameters to improve its performance on a given task.  This adjustment process is called \"training\".\n",
            "\n",
            "    *   **Types of Machine Learning:**\n",
            "\n",
            "        *   **Supervised Learning:**\n",
            "            *   **Concept:** You provide the algorithm with labeled data. Labeled data means that each input example has a corresponding \"correct\" output.\n",
            "            *   **Example:** Training an image classifier to identify cats and dogs. You would feed the algorithm thousands of images of cats and dogs, each labeled as either \"cat\" or \"dog\".\n",
            "            *   **Common Algorithms:** Linear Regression, Logistic Regression, Support Vector Machines (SVMs), Decision Trees, Random Forests, Neural Networks.\n",
            "        *   **Unsupervised Learning:**\n",
            "            *   **Concept:** You provide the algorithm with unlabeled data, and it tries to find hidden patterns, structures, or relationships within the data.\n",
            "            *   **Example:** Clustering customers into different segments based on their purchasing behavior, without knowing what those segments are in advance.\n",
            "            *   **Common Algorithms:** K-Means Clustering, Hierarchical Clustering, Principal Component Analysis (PCA), Association Rule Mining.\n",
            "        *   **Reinforcement Learning:**\n",
            "            *   **Concept:**  The algorithm learns by interacting with an environment. It receives rewards or penalties for its actions, and it learns to maximize its cumulative reward over time.\n",
            "            *   **Example:** Training a robot to walk. The robot receives a reward for moving forward and a penalty for falling down.\n",
            "            *   **Common Algorithms:** Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods.\n",
            "\n",
            "    *   **Machine Learning Process (General Outline):**\n",
            "\n",
            "        1.  **Data Collection:** Gather a relevant dataset.  The quality and quantity of data are crucial.\n",
            "        2.  **Data Preprocessing:** Clean, transform, and prepare the data for training. This might involve handling missing values, removing outliers, and scaling features.\n",
            "        3.  **Feature Engineering (Optional):** Create new features from the existing data that might be more informative for the algorithm.\n",
            "        4.  **Model Selection:** Choose an appropriate machine learning algorithm based on the type of problem, the data characteristics, and the desired outcome.\n",
            "        5.  **Model Training:** Feed the data to the algorithm, and it adjusts its internal parameters to learn the patterns in the data.\n",
            "        6.  **Model Evaluation:** Assess the performance of the trained model on a separate dataset (the \"test set\") to estimate how well it will generalize to new, unseen data.\n",
            "        7.  **Hyperparameter Tuning:** Adjust the algorithm's settings (hyperparameters) to optimize its performance.\n",
            "        8.  **Deployment:** Integrate the trained model into a real-world application.\n",
            "        9.  **Monitoring and Maintenance:** Continuously monitor the model's performance and retrain it as needed with new data.\n",
            "\n",
            "**Deep Learning: A Subfield of Machine Learning**\n",
            "\n",
            "*   **How it Works:** Deep learning uses artificial neural networks with many layers (hence \"deep\"). These networks are inspired by the structure of the human brain. Each layer learns to extract increasingly complex features from the data.\n",
            "*   **Key Idea:** Automatic feature extraction. Unlike traditional machine learning, where you often need to manually engineer features, deep learning algorithms can learn features directly from the raw data.\n",
            "*   **Example:** Image recognition, natural language processing, speech recognition.\n",
            "*   **Pros:**\n",
            "    *   Excellent performance on complex tasks, especially those involving unstructured data (images, text, audio).\n",
            "    *   Automatic feature extraction reduces the need for manual feature engineering.\n",
            "*   **Cons:**\n",
            "    *   Requires large amounts of data for training.\n",
            "    *   Computationally expensive.\n",
            "    *   Difficult to interpret (black box).  It can be hard to understand why a deep learning model makes a particular decision.\n",
            "\n",
            "**Key Components Across AI Approaches:**\n",
            "\n",
            "*   **Algorithms:**  The specific procedures or sets of rules that the AI system follows to achieve its goals.\n",
            "*   **Data:** The raw material that AI systems use to learn and make decisions. The quality, quantity, and relevance of data are critical for AI success.\n",
            "*   **Computational Power:** AI, especially machine learning and deep learning, requires significant computational resources to train models and perform complex calculations.\n",
            "*   **Software Frameworks and Libraries:**  Tools that provide pre-built functions and structures to simplify AI development (e.g., TensorFlow, PyTorch, scikit-learn).\n",
            "*   **Hardware:** Specialized hardware, such as GPUs (Graphics Processing Units) and TPUs (Tensor Processing Units), can accelerate AI computations.\n",
            "\n",
            "**Examples of AI in Action:**\n",
            "\n",
            "*   **Self-driving cars:** Use computer vision, sensor data, and machine learning to navigate roads.\n",
            "*   **Spam filters:** Use machine learning to identify and filter out unwanted emails.\n",
            "*   **Virtual assistants (Siri, Alexa, Google Assistant):** Use natural language processing and machine learning to understand and respond to voice commands.\n",
            "*   **Recommender systems (Netflix, Amazon):** Use machine learning to suggest items that users might be interested in.\n",
            "*   **Medical diagnosis:** AI systems can analyze medical images and patient data to assist doctors in diagnosing diseases.\n",
            "*   **Fraud detection:** AI systems can identify fraudulent transactions by analyzing patterns in financial data.\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "*   **Bias:** AI models can inherit biases present in the data they are trained on. This can lead to unfair or discriminatory outcomes.\n",
            "*   **Explainability:**  It can be difficult to understand why some AI models make particular decisions.  This is especially true for complex deep learning models.\n",
            "*   **Ethics:** The use of AI raises ethical concerns about privacy, security, and the potential for job displacement.\n",
            "*   **Regulation:**  Governments are beginning to develop regulations to govern the development and deployment of AI.\n",
            "\n",
            "**In Summary:**\n",
            "\n",
            "AI is a broad field that encompasses various approaches to creating intelligent computer systems. Machine learning, especially deep learning, has become a dominant approach, enabling significant advancements in areas such as computer vision, natural language processing, and robotics. However, it's important to be aware of the limitations and ethical considerations associated with AI, and to develop and deploy AI systems responsibly.\n",
            "\n",
            "I hope this explanation is helpful. Let me know if you have more questions!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=\"mii'z_API_key\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=\"Explain how AI works\",\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing model with more ques..."
      ],
      "metadata": {
        "id": "Kar4sjs8v3vq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c0d63e-7ca3-4fdd-e56f-7c811e1acb94",
        "id": "S50v55WJjbq9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RGB stands for **Red, Green, and Blue**. It's a color model that describes a color by specifying the intensity of these three primary colors of light. Essentially, it's how your computer screen, TV, and phone display colors.\n",
            "\n",
            "Here's a breakdown:\n",
            "\n",
            "* **The Concept:**  Imagine three spotlights, one red, one green, and one blue.  By varying the brightness (intensity) of each spotlight and shining them onto the same spot, you can create a wide range of colors.  When all three are at their maximum intensity, you get white.  When all three are off (zero intensity), you get black.\n",
            "\n",
            "* **How it Works (Numerically):**  In digital systems, each color component (Red, Green, Blue) is typically represented by a number, usually ranging from 0 to 255.\n",
            "\n",
            "    * **0** means that color component is completely off (no light).\n",
            "    * **255** means that color component is at its maximum brightness.\n",
            "\n",
            "* **Examples:**\n",
            "\n",
            "    * **Red:** (255, 0, 0) - Full red, no green, no blue.\n",
            "    * **Green:** (0, 255, 0) - No red, full green, no blue.\n",
            "    * **Blue:** (0, 0, 255) - No red, no green, full blue.\n",
            "    * **White:** (255, 255, 255) - Full red, full green, full blue.\n",
            "    * **Black:** (0, 0, 0) - No red, no green, no blue.\n",
            "    * **Yellow:** (255, 255, 0) - Full red, full green, no blue.  (Red + Green = Yellow)\n",
            "    * **Magenta:** (255, 0, 255) - Full red, no green, full blue. (Red + Blue = Magenta)\n",
            "    * **Cyan:** (0, 255, 255) - No red, full green, full blue. (Green + Blue = Cyan)\n",
            "    * **Gray:** (128, 128, 128) - Equal amounts of red, green, and blue in a medium intensity.  Different levels of equal intensity create different shades of gray.\n",
            "\n",
            "* **Why RGB?**\n",
            "\n",
            "    * **Additive Color Mixing:** RGB is an *additive* color model.  This means that you start with black (no light) and add light (red, green, and blue) to create other colors. This is in contrast to subtractive color models like CMYK (used in printing), where you start with white (paper) and subtract colors (cyan, magenta, yellow, black) to create other colors.\n",
            "    * **Digital Displays:**  RGB is the native color model for most digital displays because they create colors by emitting light.\n",
            "\n",
            "* **Representations:** RGB values can be represented in different ways:\n",
            "\n",
            "    * **RGB(red, green, blue):** e.g., RGB(255, 0, 0)\n",
            "    * **Hexadecimal (Hex Code):**  A six-digit hexadecimal number representing the RGB values. Each pair of digits represents one color component. For example, #FF0000 is the hexadecimal equivalent of RGB(255, 0, 0).  #FF = 255, #00 = 0.\n",
            "\n",
            "* **In summary, RGB is a fundamental color model used for displaying colors on digital devices. It works by mixing red, green, and blue light in varying intensities to create a wide spectrum of colors.**\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=\"-----\")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=\"Explain what are RGB colors\",\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "same work as aforesaid....just another way with more parameters!"
      ],
      "metadata": {
        "id": "R1SrDPL1v87f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# 🔹 Configure API Key (Make sure it's correct!)\n",
        "genai.configure(api_key=\"-----------\")\n",
        "\n",
        "# 🔹 Initialize Gemini Model\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")  # Use \"gemini-1.5-flash\" or \"gemini-1.5-pro\"\n",
        "\n",
        "# 🔹 Generate response with parameters\n",
        "response = model.generate_content(\n",
        "    \"Explain what are RGB colors\",\n",
        "    generation_config={\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.9,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 512\n",
        "    }\n",
        ")\n",
        "\n",
        "# 🔹 Print AI Response\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "_lO4F5YwlHmE",
        "outputId": "9a40d333-3fdb-4773-81a1-b868020035ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RGB colors are a fundamental concept in digital displays and image creation. RGB stands for **Red, Green, and Blue**, and it's an **additive color model**. This means that colors are created by adding different amounts of red, green, and blue light together.\n",
            "\n",
            "Here's a breakdown:\n",
            "\n",
            "*   **Additive Color Model:**  Unlike subtractive color models (like CMYK used in printing), where colors are created by *subtracting* light from white, RGB *adds* light to create colors. Think of it like mixing colored spotlights.\n",
            "\n",
            "*   **Primary Colors:** Red, green, and blue are the primary colors in this model.\n",
            "\n",
            "*   **How it Works:**\n",
            "    *   Each color (red, green, and blue) is assigned a numerical value, typically ranging from 0 to 255.\n",
            "    *   **0** represents the absence of that color (no light).\n",
            "    *   **255** represents the maximum intensity of that color (full light).\n",
            "    *   By varying the values of red, green, and blue, you can create a wide range of colors.\n",
            "\n",
            "*   **Examples:**\n",
            "    *   **Black:** (0, 0, 0) - No red, no green, no blue.  Complete darkness.\n",
            "    *   **White:** (255, 255, 255) - Full red, full green, full blue.  Maximum light.\n",
            "    *   **Red:** (255, 0, 0) - Full red, no green, no blue.\n",
            "    *   **Green:** (0, 255, 0) - No red, full green, no blue.\n",
            "    *   **Blue:** (0, 0, 255) - No red, no green, full blue.\n",
            "    *   **Yellow:** (255, 255, 0) - Full red, full green, no blue. (Red + Green = Yellow)\n",
            "    *   **Cyan:** (0, 255, 255) - No red, full green, full blue. (Green + Blue = Cyan)\n",
            "    *   **Magenta:** (255, 0, 255) - Full red, no green, full blue. (Red + Blue = Magenta)\n",
            "    *   **Gray:** (12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model recognizing images"
      ],
      "metadata": {
        "id": "9S7nIdpvwFyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import base64\n",
        "\n",
        "# Initialize the client\n",
        "API_KEY = \"-------------\"  # Replace with your actual API key\n",
        "genai.configure(api_key=API_KEY)  # Configure the API key\n",
        "\n",
        "# Initialize the model\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")  # Use the correct model for image recognition\n",
        "\n",
        "# Function to encode an image to base64\n",
        "def encode_image_to_base64(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"/content/AI_MONSTER.jpg\"  # Replace with the path to your image\n",
        "\n",
        "# Encode the image to base64\n",
        "base64_image = encode_image_to_base64(image_path)\n",
        "\n",
        "# Create the content for the API request\n",
        "contents = [\n",
        "    {\n",
        "        \"parts\": [\n",
        "            {\"text\": \"What is in this image?\"},  # Text prompt\n",
        "            {\"inline_data\": {\"mime_type\": \"image/jpg\", \"data\": base64_image}},  # Image data\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Call the API\n",
        "response = model.generate_content(contents)  # Correct method call\n",
        "\n",
        "# Print the response\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "L-mN5wFepkBa",
        "outputId": "c6799179-2937-4075-f470-90ef170edf49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image features a large, monstrous creature standing in what appears to be a cityscape at night. The creature is predominantly black and has a reptilian or demonic appearance, with sharp teeth, glowing orange eyes, and spikes along its back and head. There are glowing orange spots on its body, possibly indicating some form of energy or bioluminescence. The buildings in the background are tall skyscrapers with illuminated windows, suggesting a modern urban setting. The overall tone of the image is dark and ominous.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model recognizing video in frames--BUT--its an impractical approach!"
      ],
      "metadata": {
        "id": "TufHKg0cwONK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import base64\n",
        "import cv2\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Set up API key and model\n",
        "API_KEY = \"----------\"  # Replace with your API key\n",
        "genai.configure(api_key=API_KEY)\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")  # Use the correct model\n",
        "\n",
        "# Function to download a video from a URL (if needed)\n",
        "def download_video(url, save_path):\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "        print(f\"Video downloaded and saved to {save_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download video. Status code: {response.status_code}\")\n",
        "\n",
        "# Function to extract frames from a video\n",
        "def extract_frames(video_path, frame_interval=10):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return frames\n",
        "\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_count % frame_interval == 0:  # Extract every nth frame\n",
        "            frames.append(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# Function to encode frame to base64\n",
        "def encode_frame(frame):\n",
        "    _, buffer = cv2.imencode(\".jpg\", frame)\n",
        "    return base64.b64encode(buffer).decode(\"utf-8\")\n",
        "\n",
        "# Path to your video file (with or without extension)\n",
        "video_input = \"/content/WhatsApp Video 2025-02-27 at 6.56.00 PM.mp4\"  # Replace with your video file path (no extension needed)\n",
        "\n",
        "# Check if the input is a URL\n",
        "if video_input.startswith((\"http://\", \"https://\")):\n",
        "    # Download the video from the URL\n",
        "    video_path = \"downloaded_video\"  # No extension needed\n",
        "    download_video(video_input, video_path)\n",
        "else:\n",
        "    # Use the local video file\n",
        "    video_path = video_input\n",
        "\n",
        "# Extract frames from the video\n",
        "frames = extract_frames(video_path, frame_interval=10)  # Adjust frame_interval as needed\n",
        "\n",
        "# Analyze each frame\n",
        "for i, frame in enumerate(frames):\n",
        "    # Encode the frame\n",
        "    base64_frame = encode_frame(frame)\n",
        "\n",
        "    # Prepare the content\n",
        "    contents = [\n",
        "        {\n",
        "            \"parts\": [\n",
        "                {\"text\": \"What is happening in this video frame?\"},  # Your prompt\n",
        "                {\"inline_data\": {\"mime_type\": \"image/jpg\", \"data\": base64_frame}},  # Frame data\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Get the response\n",
        "    response = model.generate_content(contents)\n",
        "\n",
        "    # Print the result for this frame\n",
        "    print(f\"Frame {i + 1} Analysis:\")\n",
        "    print(response.text)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Clean up downloaded video (if applicable)\n",
        "if video_input.startswith((\"http://\", \"https://\")):\n",
        "    os.remove(video_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qmyn498Jttbw",
        "outputId": "c81e3fb2-afb2-401d-d920-6d2d2860698e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame 1 Analysis:\n",
            "In the video frame, two individuals, a child and an older individual (possibly a sibling or parent), are outside, sitting on a striped mat on the grass. They appear to be painting. A flowerpot is visible, already decorated with painted designs. Both individuals have paint and brushes. The child is painting a small object, while the older individual seems to be working on a different painting project, likely either the flower pot or another object just out of view.\n",
            "--------------------------------------------------\n",
            "Frame 2 Analysis:\n",
            "In the video frame, two children appear to be engaged in an art activity outdoors. They are painting a terracotta pot with various colors. They are sitting on a striped mat on the grass and have pots of paint and paintbrushes.\n",
            "--------------------------------------------------\n",
            "Frame 3 Analysis:\n",
            "In the video frame, two children are outdoors, possibly in a garden, painting flower pots. They are sitting on a striped mat and using paintbrushes and several small containers of paint. One child is painting a pot, while the other is painting a larger, undecorated object.\n",
            "--------------------------------------------------\n",
            "Frame 4 Analysis:\n",
            "In the video frame, two children are sitting on a striped mat in a grassy outdoor setting, possibly a backyard. They appear to be painting flower pots. One child is sitting upright, wearing a red patterned shirt, while the other is lying on their stomach, wearing a white long-sleeved shirt. There are several pots nearby, some of which already have paint on them in stripes of blue and yellow. Paint pots and brushes are scattered around, suggesting an art project in progress.\n",
            "--------------------------------------------------\n",
            "Frame 5 Analysis:\n",
            "In the image, a young girl and a child are outdoors, likely in a garden or backyard, and they are engaged in painting flowerpots. They are sitting on a striped mat, and there are pots, paint containers, and brushes around them, indicating they are in the middle of a creative activity. The pots are being decorated with blue and yellow paint.\n",
            "\n",
            "--------------------------------------------------\n",
            "Frame 6 Analysis:\n",
            "In the video frame, two children are sitting on a mat outside, painting a terracotta pot. They have various colors of paint available and are using paintbrushes to decorate the pot with blue and yellow stripes. It appears to be a creative and artistic activity taking place in a garden or backyard setting.\n",
            "--------------------------------------------------\n",
            "Frame 7 Analysis:\n",
            "In the video frame, two children are painting a flower pot. They are sitting on a mat outside on the grass. There are pots of paint and paintbrushes visible. The flower pot is already partially decorated with blue and yellow paint. The child is applying the blue color to the flower pot with a brush.\n",
            "--------------------------------------------------\n",
            "Frame 8 Analysis:\n",
            "In the video frame, two children are painting terracotta flower pots outdoors. They are sitting on a striped mat in a grassy area, with various pots and paint containers around them. One child appears to be an older girl, possibly a teenager, while the other is a younger child. They are both using paintbrushes to decorate the pots with different colors.\n",
            "--------------------------------------------------\n",
            "Frame 9 Analysis:\n",
            "In the video frame, two children are outside, sitting on a striped mat in the grass, and painting. They have various small containers of paint around them. One child appears to be painting a flower pot with blue and yellow stripes, while the other child is working on something else, possibly also a flower pot.\n",
            "\n",
            "--------------------------------------------------\n",
            "Frame 10 Analysis:\n",
            "In the video frame, two children are painting terracotta pots with bright colors. They are sitting on a striped mat on a grassy area. One child is painting vertical stripes on the pot with blue and yellow paint, while the other is painting a smaller pot nearby. There are several small paint containers on the mat, filled with different colored paints.\n",
            "--------------------------------------------------\n",
            "Frame 11 Analysis:\n",
            "In the video frame, two children, likely siblings, are outdoors, sitting on a mat on the grass, and painting plant pots. One child is lying down while painting, and the other is sitting up. Paint pots in various colors are nearby. One of the painted pots is visible in the foreground, adorned with blue and yellow stripes and a \"V\" shape. It seems like they are engaged in a fun arts and crafts activity together.\n",
            "\n",
            "--------------------------------------------------\n",
            "Frame 12 Analysis:\n",
            "In the video frame, two people appear to be painting outdoors, possibly on a lawn or grassy area. They are using paintbrushes and small containers of paint, and it looks like they are decorating a large flowerpot. One person is sitting up, while the other is lying on their stomach. There are painted sections visible on the pot, suggesting they have already started their project.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ReadTimeout",
          "evalue": "HTTPConnectionPool(host='localhost', port=37785): Read timed out. (read timeout=600.0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d6c41bb2e011>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Print the result for this frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             _retry_error_helper(\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             response = GenerativeServiceRestTransport._GenerateContent._get_response(\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(host, metadata, query_params, session, timeout, transcoded_request, body)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             response = getattr(session, method)(\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0;34m\"{host}{uri}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    542\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    711\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_InvalidHeader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mReadTimeout\u001b[0m: HTTPConnectionPool(host='localhost', port=37785): Read timed out. (read timeout=600.0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model recognizing video--perfect way!"
      ],
      "metadata": {
        "id": "c6yM6zNMwdNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import base64\n",
        "\n",
        "# Initialize the client\n",
        "API_KEY = \"---------\"  # Replace with your actual API key\n",
        "genai.configure(api_key=API_KEY)  # Configure the API key\n",
        "\n",
        "# Initialize the model\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")  # Use the correct model\n",
        "\n",
        "# Function to encode a video to base64\n",
        "def encode_video_to_base64(video_path):\n",
        "    with open(video_path, \"rb\") as video_file:\n",
        "        return base64.b64encode(video_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# Path to your video\n",
        "video_path = \"/content/WhatsApp Video 2025-02-27 at 6.56.00 PM.mp4\"  # Replace with the path to your video\n",
        "\n",
        "# Encode the video to base64\n",
        "base64_video = encode_video_to_base64(video_path)\n",
        "\n",
        "# Create the content for the API request\n",
        "contents = [\n",
        "    {\n",
        "        \"parts\": [\n",
        "            {\"text\": \"What is happening in this video?\"},  # Text prompt\n",
        "            {\"inline_data\": {\"mime_type\": \"video/mp4\", \"data\": base64_video}},  # Video data\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Call the API\n",
        "response = model.generate_content(contents)  # Correct method call\n",
        "\n",
        "# Print the response\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "NPqwmVh3t63U",
        "outputId": "1751df29-9a99-4d8d-cc8c-eb6bff2006a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the video, a young girl and a younger boy are sitting on a striped mat on the grass and painting a flower pot and a concrete post outside on the grass.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBwhkdUD8h1J",
        "outputId": "9eb1e83e-b31f-4298-ac5b-ea1054cf4a6a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.14.1-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Downloading SpeechRecognition-3.14.1-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD6tpBRf-AUy",
        "outputId": "2740a21f-e541-420a-880a-20c3979b74b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.11/dist-packages (3.14.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model extracting audio info"
      ],
      "metadata": {
        "id": "Ue9TAOQ0Nvz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "import speech_recognition as sr\n",
        "from bs4 import BeautifulSoup\n",
        "import base64\n",
        "\n",
        "# Initialize the Gemini client\n",
        "client = genai.Client(api_key=\"--------\")\n",
        "\n",
        "# Function to process WhatsApp HTML file\n",
        "def process_whatsapp_audio(html_file_path):\n",
        "    try:\n",
        "        # Step 1: Read the HTML file\n",
        "        with open(html_file_path, \"r\") as file:\n",
        "            soup = BeautifulSoup(file, \"html.parser\")\n",
        "\n",
        "        # Step 2: Find the <audio> tag\n",
        "        audio_tag = soup.find(\"audio\")\n",
        "        if not audio_tag or \"src\" not in audio_tag.attrs:\n",
        "            return \"Error: No <audio> tag found in the HTML file.\"\n",
        "\n",
        "        # Step 3: Extract base64 audio data\n",
        "        audio_src = audio_tag[\"src\"]\n",
        "        if not audio_src.startswith(\"data:audio\"):\n",
        "            return \"Error: Audio is not embedded as base64.\"\n",
        "\n",
        "        audio_data = audio_src.split(\",\")[1]\n",
        "        audio_bytes = base64.b64decode(audio_data)\n",
        "\n",
        "        # Step 4: Save the audio as a temporary WAV file\n",
        "        with open(\"temp_audio.wav\", \"wb\") as audio_file:\n",
        "            audio_file.write(audio_bytes)\n",
        "\n",
        "        # Step 5: Transcribe the audio\n",
        "        recognizer = sr.Recognizer()\n",
        "        with sr.AudioFile(\"temp_audio.wav\") as source:\n",
        "            audio = recognizer.record(source)\n",
        "            transcription = recognizer.recognize_google(audio)\n",
        "\n",
        "        # Step 6: Analyze the transcription\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            contents=transcription,\n",
        "        )\n",
        "        return transcription, response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "html_file_path = \"/content/(2) WhatsApp.html\"  # Replace with your file path\n",
        "result = process_whatsapp_audio(html_file_path)\n",
        "\n",
        "if isinstance(result, tuple):\n",
        "    transcription, analysis = result\n",
        "    print(\"Transcription:\", transcription)\n",
        "    print(\"Analysis:\", analysis)\n",
        "else:\n",
        "    print(result)  # Print error message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_phzJp1AMO1",
        "outputId": "f0933162-254f-4f51-8d99-fe1a3cb651b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: No <audio> tag found in the HTML file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XALCwJYxCTLR",
        "outputId": "a9901528-5cf6-4d16-b045-b6f0722eebc8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model reading PDF file"
      ],
      "metadata": {
        "id": "K1Ie-IQgN7t8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "import PyPDF2\n",
        "\n",
        "# Initialize the Gemini client\n",
        "client = genai.Client(api_key=\"mii_API_key\")\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_file_path):\n",
        "    with open(pdf_file_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "# Function to analyze text using Gemini API\n",
        "def analyze_text(text):\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        contents=text,\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "# Main function\n",
        "def main(pdf_file_path):\n",
        "    # Step 1: Extract text from the PDF\n",
        "    text = extract_text_from_pdf(pdf_file_path)\n",
        "    print(\"Extracted Text:\", text)\n",
        "\n",
        "    # Step 2: Analyze the text using Gemini API\n",
        "    analysis = analyze_text(text)\n",
        "    print(\"Analysis:\", analysis)\n",
        "\n",
        "# Example usage\n",
        "pdf_file_path = \"/content/Document (4).pdf\"  # Replace with your PDF file path\n",
        "main(pdf_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEdvaLwQBM5b",
        "outputId": "e4e1fceb-1749-48c4-a9fb-53effdb88473"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text: CAUSES: - \n",
            "The COVID -19 virus, caused by the novel coronavirus \n",
            "SARS-CoV-2, primarily spread through respiratory droplets \n",
            "when an infected person coughs, sneezes, or talks. The \n",
            "main causes of transmission include:  \n",
            "1.Person-to-Person Contact: Close contact with \n",
            "infected individuals increases the risk of \n",
            "transmission.  \n",
            "2.Respiratory Droplets: When an infected person \n",
            "coughs, sneezes, or talks, tiny droplets containing the \n",
            "virus can spread in the air and infect others.  \n",
            "3.Surface Contact: Touching contaminated surfaces \n",
            "and then touching the face (especially the mouth, \n",
            "nose, or eyes) can lead to infection.  \n",
            "4.Airborne Transmission:  In some cases, the virus can \n",
            "remain airborne in enclosed spaces with poor \n",
            "ventilation, posing a risk of transmission.  \n",
            "5.Variants and Mutations: Mutations in the virus may \n",
            "affect its transmissibility and ability to infect hosts, \n",
            "contributing to the spread of different variants.  \n",
            " \n",
            " \n",
            " \n",
            "Analysis: This is a good, concise summary of the causes of COVID-19 transmission. It's well-organized and covers the key factors. Here's a slightly more refined version that emphasizes some points and might be useful in different contexts:\n",
            "\n",
            "**CAUSES OF COVID-19 TRANSMISSION**\n",
            "\n",
            "The COVID-19 pandemic is driven by the SARS-CoV-2 virus, a novel coronavirus primarily spread through respiratory droplets produced when an infected person coughs, sneezes, or talks. Key factors contributing to transmission include:\n",
            "\n",
            "*   **Close Person-to-Person Contact:** Proximity to infected individuals is a significant risk factor. The closer and more prolonged the contact, the higher the likelihood of transmission.\n",
            "\n",
            "*   **Respiratory Droplet Spread:** The virus is primarily spread through respiratory droplets expelled during coughing, sneezing, talking, and even breathing. These droplets can land in the mouths or noses of people who are nearby or possibly be inhaled into the lungs.\n",
            "\n",
            "*   **Contaminated Surfaces (Fomite Transmission):** While less significant than droplet spread, the virus can survive on surfaces for varying lengths of time. Touching these contaminated surfaces and then touching the face (eyes, nose, or mouth) can lead to infection.\n",
            "\n",
            "*   **Airborne Transmission (Aerosols):** In poorly ventilated, enclosed spaces, smaller, lighter particles called aerosols containing the virus can remain suspended in the air for longer periods. This increases the risk of infection, especially in crowded indoor settings.\n",
            "\n",
            "*   **Viral Variants and Mutations:** The virus is constantly evolving. New variants may exhibit increased transmissibility, making them spread more easily and potentially leading to surges in infections.\n",
            "\n",
            "**Key Improvements/Explanations in the Refined Version:**\n",
            "\n",
            "*   **Emphasis on Droplets:** The revised version emphasizes that droplet spread is considered the **primary** mode of transmission. While surface and airborne transmission play a role, droplets are the main concern.\n",
            "*   **Clarification of Airborne Transmission:** Clarifies that it is especially in poorly ventilated spaces that it can occur.\n",
            "*   **\"Proximity and Duration of Contact\":** Emphasizes the significance of both how close and how long someone is in contact with an infected individual.\n",
            "\n",
            "This refined version provides a more comprehensive and nuanced understanding of COVID-19 transmission causes.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}